<!doctype html>
<html lang="en">

<head>
  <title>Thinking Like Transformers</title>
  <meta property="og:title" content="Thinking Like Transformers" />
  <meta name="twitter:title" content="Thinking Like Transformers" />
  <meta name="description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta property="og:description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta name="twitter:description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr"><a href="https://arxiv.org/abs/2106.06981" target="_blank">Thinking Like Transformers</a>
        </nobr>
        <!-- <nobr class="widenobr">For CS 7150</nobr> -->
      </h1>
    </div>
    <address>
      <nobr><a href="https://gailweiss.github.io/" target="_blank">Gail Weiss</a>,</nobr>
      <nobr><a href="https://u.cs.biu.ac.il/~yogo/" target="_blank">Yoav Goldberg</a>,</nobr>
      <nobr><a href="https://csaws.cs.technion.ac.il/~yahave/" target="_blank">Eran Yahav</a></nobr>
    </address>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <!-- <h2>An Analysis of [papername]</h2> -->
        <!-- <p>Describe the paper and the big question about it that interests you.</p> -->
        Analysis by <a href="" target="_blank">David Atkinson</a> and <a href="https://arnab-api.github.io/"
          target="_blank">Arnab Sen Sharma</a>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <!-- <h2>Literature Review; Biography; Social Impact; Industry Applications; Follow-on Research; and Peer-Review</h2>

        <p>Just as we have done in the role-playing exercise, analyze the paper from all perspectives.
        </p>

        <p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on
          your findings.
        </p> -->
        <h2> Introduction/Motivation </h2>
        <p style="text-align: justify;">
        Transformer models are famously opaque, made up of millions—sometimes billions—of uninterpretable parameters.
        It's natural to ask if there is a better representation of these parameters.
        <i>Thinking Like Transformers</i> argues that sometimes the answer is "yes," and that the better representation takes the form of a programming language called RASP, short for Restricted Access Sequence Processing Language.
        </p><p style="text-align: justify;">
        RASP is a simple language designed to be “compiled” to transformer weights.
        Like transformers, RASP programs take sequences as input and produce different sequences as output.
        Again like transformers, the structure of the RASP language is three-fold:
        </p><p style="text-align: justify;">
        First, every program starts with tokens (the input to a program, of the kind you might see in a context window) and indices (a sequence of integers between <i>0</i> and <i>n-1</i>, where <i>n</i> is the length of tokens).
        By analogy to the tokens and positional embeddings of a transformer, the tokens and indices of a RASP program combine to give sequential structure to user input.
        </p><p style="text-align: justify;">
        The next kind of RASP program component is the element-wise operation, analogous to the MLP components of trained transformer.
        These will be familiar: we can use <span style="font-family: 'Courier New', Courier, monospace;"><b>+</b></span> to add two sequences; <span style="font-family: 'Courier New', Courier, monospace;"><b>pow(., .)</b></span> to exponentiate each element in a sequence; <span style="font-family: 'Courier New', Courier, monospace;"><b>&lt;</b></span>,<span style="font-family: 'Courier New', Courier, monospace;"><b>&gt;</b></span>,<span style="font-family: 'Courier New', Courier, monospace;"><b>&lt;=</b></span>,<span style="font-family: 'Courier New', Courier, monospace;"><b>&gt;=</b></span>, and <span style="font-family: 'Courier New', Courier, monospace;"><b>==</b></span> to express predicates over two elements; and so on.
        </p><p style="text-align: justify;">
        Most importantly, we also have an attention mechanism analogue: the <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> and <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span> operations. Although a fuller description can be found <a href="#diagram">below</a>, at a high level, <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> combines two sequences with a predicate, generating a matrix containing the result of applying the predicate to  every possible combination of sequence elements. <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>, subsequently, will combine that resulting matrix with a third sequence to produce a new sequence. If you squint, you can see how the key and query vectors of the transformer's attention mechanism are treated similarly to the inputs to <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span>, and how the value vectors of the transformer are analogous to the the sequence input of <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>.
        </p><p style="text-align: justify;">
        What can you actually do with RASP? A few things. First, having constructed a RASP program, you have also established an upper bound on the number of layers and attention heads needed to express your algorithm. Second, and more speculatively, you might think you've improved your intuitive sense of what a trained model might be doing internally when it performs the task expressed by your program. (The authors find that a trained model will sometimes, but not always, embody the algorithm implied by the equivalent RASP program.) Finally, RASP can be used as "laboratory for interpretability" [1 (Linder 2023)]. When trained transformer models are interpreted, it is often unclear when or how full success is achieved. Using RASP programs as interpretation targets can serve as a ground truth for interpretability methods. [2 (Conmy 2023)]
        </p>


        <h2 id="diagram"> <b>R</b>estricted <b>A</b>ccess <b>S</b>equence <b>P</b>rocessing (RASP) </h2>

        <br>
        <h4> Analogues to components in a transformer block</h4>

        <figure>
          <img src="images/rasp_ops.png" class="center_image" style="width: 45%;">
          <figcaption>Caption: Components of a Transformer block and their RASP analogues.</figcaption>
        </figure>

        <br>
        <h5> Sequence Operators (S-Ops)</h5>
        <p style="text-align: justify;">

          S-ops in RASP are operations that take a sequence as input and generate a sequence of the same length as
          output. Functions of MLP and attention blocks in a transformer are emulated as S-Ops in RASP. RASP also
          provides two built-in S-Ops: <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              <span style="font-family: 'Courier New', Courier, monospace;">
                &nbsp; tokens("hello") = [h, e, l, l, o] <br>
                &nbsp; indices("hello") = [0, 1, 2, 3, 4] <br>
              </span>
            </p>
          </div>
        </div>

        </p>

        <br>
        <h5> Element-wise Operators </h5>
        <p style="text-align: justify;">

          These operations are analogous to element-wise transformations performed by an MLPs.
        <figure>
          <img src="images/mlp_op.png" class="center_image" style="width: 35%;">
        </figure>
        <br>

        An example of such operations will be: <br><br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              <span style="font-family: 'Courier New', Courier, monospace;">
                &nbsp; 3 * indices("hello") = [0, 3, 6, 9, 12] <br>
              </span>
            </p>
          </div>
        </div>

        </p>

        <br>
        <h5> Select and Aggregate Operations </h5>
        <p style="text-align: justify;">
          RASP emulates the mechanism of a single attention head with a combination of
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span> and
          <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate()</b></span>
          operations.
        </p>

        <p style="text-align: justify;">
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span>
          takes two sequences; the
          <span style="color: darkorange; font-family: 'Courier New', Courier, monospace;"><b>key</b></span>
          and the
          <span style="color: teal; font-family: 'Courier New', Courier, monospace;"><b>query</b></span>
          as input, and generates a <em>binary</em> mask using
          them. In RASP, this binary mask is called a <em>selector</em>. <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              &nbsp &nbsp
              <span style="font-family: 'Courier New', Courier, monospace;">
                <span style="color: palevioletred;"><b>select(</b></span>
                <span style="color: darkorange;"><b>key</b></span>
                <span style="color: palevioletred;"><b>,</b></span>
                <span style="color: teal;"><b>query</b></span>
                <span style="color: palevioletred;"><b>,</b></span>
                <b>binary op</b>
                <span style="color: palevioletred;"><b>)</b></span>
              </span>
            </p>
          </div>
        </div>

        <br><br>
        The <em>selector</em> mask is be a square matrix with binary values that is analogous to the attention
        pattern of a single attention head.<br>

        The value in cell <span style="font-family: 'Courier New', Courier, monospace;">(i, j)</span> of
        <em>seletor</em> is
        decided as:

        &nbsp
        <span style="font-family: 'Courier New', Courier, monospace;">
          <span style="color: black;"><b>selector(i, j) = </b></span>
          <span style="color: teal;"><b>query(i)</b></span>
          <span style="color: black;"><b>op</b></span>
          <span style="color: darkorange;"><b>key(j)</b></span>
        </span>
        <br><br>

        To illustrate this,
        <span style="font-family: 'Courier New', Courier, monospace;">
          <span style="color: palevioletred;"><b>select(</b></span>
          <span style="color: darkorange;"><b>key = [1, 2, 3, 4, 5]</b></span>
          <span style="color: palevioletred;"><b>,</b></span>
          <span style="color: teal;"><b>query = [0, 1, 2, 3, 4]</b></span>
          <span style="color: palevioletred;"><b>,</b></span>
          <b>>=</b>
          <span style="color: palevioletred;"><b>)</b></span>
        </span>
        will generate the following <em>selector</em> - <br>

        <figure>
          <img src="images/selector_example.png" class="center_image" style="width: 20%;">
        </figure>

        In RASP, <span
          style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span> is always
        followed by an
        <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate()</b></span>
        operation, which
        takes a
        <span style="font-family: 'Courier New', Courier, monospace;"><b>selector</b></span> matrix and a
        <span style="color: red; font-family: 'Courier New', Courier, monospace;"><b>value</b></span> sequence as input,
        like below - <br> <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              &nbsp &nbsp
              <span style="font-family: 'Courier New', Courier, monospace;">
                <span style="color: darkred;"><b>aggregate(</b></span>
                <b>selector</b>
                <span style="color: darkred;"><b>,</b></span>
                <span style="color: red;"><b>value</b></span>
                <span style="color: darkred;"><b>)</b></span>
              </span>
            </p>
          </div>
        </div>

        <br>

        And, generates a sequence of the same length by aggregating the values in the <span
          style="color:red; font-family: 'Courier New', Courier, monospace;"><b>value</b></span> sequence as per
        the <em>mixture</em>
        defined in the rows of the <span style="font-family: 'Courier New', Courier, monospace;"><b>selector</b></span>
        matrix. <br>

        <br>
        <figure>
          <img src="images/aggregate_eqn.png" class="center_image" style="width: 80%;">
        </figure>
        <br>

        From here on, we will borrow the following concise diagram from <a href="https://github.com/srush/raspy"
          target="_blank"><u>RASPy</u></a> to illustrate how a single attention head is
        emulated
        using a pair of
        <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> -
        <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>
        operations. <br>

        <br>
        <figure>
          <img src="images/rasp_attn.png" class="center_image" style="width: 65%;">
        </figure>
        <br>

        </p>

        <div class="alert error">
          <span class="alertText" style="color:darkred;">
            ** A RASP selector is a square matrix of binary values, whereas an attention pattern has
            continuous values. Also, in an attention pattern, values in a row must sum to 1 due to the softmax
            applied on scaled attention. RASP doesn't impose this on a selector either.</li>
          </span>
        </div>

        <div class="alert warning">
          <span class="alertText" style="color:black; text-align: left;">
            ** In RASP, values are aggregated as the mean. However, in this analysis we show aggregation as sum,
            similar to the implementation of <a href="https://github.com/srush/raspy" target="_blank"
              style="color: darkblue;"><i><u>RASPy</u></i></a>.
            </ul>
          </span>
        </div>



        <br>
        <h4> Some Illustrative Examples </h4>

        <br>
        <h5> Example 1: Count the number of times a character c appears in a sequence S </h5>

        Input Format: <span style="font-family:'Courier New'">
          &ltsequence&gt-&ltc&gt
        </span> <br>

        RASP Program:
        <br>
        <br>
        <figure>
          <img src="images/char_rasp.png" style="width: 50%;">
        </figure>
        <br>

        <p style="text-align: justify;">
          The figure below illustrates each step for an input <span style="font-family:'Courier New'">"abaa-a"</span>.
          This implementation will require 3 transformer
          layers. Each of the attention and MLP operations will add one set of new information to the residual streams.
          The answer will be written after Attn_3.
        </p>

        <br>
        <figure>
          <img src="images/char_occurance.png" class="center_image" style="width: 100%;">
        </figure>
        <br>

        <br>
        <h5> Example 2: Find the minimum element in a sequence </h5>

        Idea: Given a sequence <span style="font-family:'Courier New'">S = [3, 2, 3, 5, 2]</span>, we will calculate 2
        counts as follows -

        <ol>
          <li>
            For each <span style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>,
            count the
            number of elements in S that are less than <span
              style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>
            <figure>
              <img src="images/count_less.png" style="width: 30%;">
            </figure>
          </li>
          <li>
            Also, count the number of elements that appear before <span
              style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span> and is equal to
            <span style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>
            <figure>
              <img src="images/count_before_eq.png" style="width: 40%;">
            </figure>
          </li>
        </ol>

        For the example sequence, <span style="font-family:'Courier New'">S = [3, 2, 3, 5, 2]</span>, the counts will
        be<br>

        <br>
        <span style="font-family:'Courier New'">count_less = [2, 0, 2, 4, 0]</span> <br>
        <span style="font-family:'Courier New'">count_before_eq = [0, 0, 1, 0, 1]</span> <br>
        And, <span style="font-family:'Courier New'">count = count_less + count_before_eq = [2, 0, 3, 4, 1]</span> <br>
        <br>

        There will only be one zero in the <span style="font-family:'Courier New'">count</span> sequence and it will be
        on the index where the minimum element appears first in S.<br>
        The RASP code with operations and updates at each step is illustrated below.

        <br>
        <br>
        <figure>
          <img src="images/minimum.png" class="center_image" style="width: 70%;">
        </figure>
        <br>


        <br>
        <br>
        <h4> Compiling RASP into a transformer model </h4>
        <p style="text-align: justify;">
          Once we have a RASP program that solves a certain task, it is possible to compile that RASP program into an
          actual transformer model. The longest path in the RASP program from input to output decides the number of
          transformer layers. And, the number of independent
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> -
          <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>
          operations being done in parallel decides the number of attention heads per layer.
        </p>
        <p style="text-align: justify;">
          After we decide on a transformer architecture, the model can be trained with supervision from -
        <ol>
          <li>An dataset with input-output pairs that demonstrates the task, how any supervised model is usually
            trained.</li>
          <li>Attention patterns of each of the heads are matched to a corresponding <em>selector</em> pattern
            in the RASP program.</li>
        </ol>

        <figure>
          <img src="images/compiling.png" class="center_image" style="width: 90%;">
          <figcaption>(a) shows the RASP program that solves the double histogram problem. The program uses
            three
            attention operators. (b) shows the selector pattern for the 3 attention operations for an input
            sequence
            <span style="font-family:'Courier New'">"§aaabbccdef"</span>. (c) shows the corresponding attention
            heatmaps
            for a transformer model trained to solve the
            task with the attention pattern specified by the selectors of the RASP program.
          </figcaption>
        </figure>

        Two followup works by <a href="https://arxiv.org/pdf/2301.05062.pdf" target="_blank">Lindner et al
          2023</a> and
        <a href="https://arxiv.org/pdf/2306.01128.pdf" target="_blank">Friedman et al 2023</a> presented
        compilers that
        generate a transformer
        model given a RASP program and a set of input-output demonstration pairs.

        </p>

        <h2> Related Works </h2>

        <h2> Applications </h2>
        <br>
        <h4> Testing Interpretability Tools </h4>
        <p style="text-align: justify;">
          Interpretability is a budding field that seeks to understand how neural networks work by <em>decompiling</em>
          them into human-readable algorithm or program. Weiss et al. here take an opposite approach in this paper -
          solving a particular problem with RASP that only allows operations analogous to different components of a
          transformer. This RASP solution can be <em>compiled</em> into a transformer model that is interpretable
          by design. These interpretable toy models may not be very useful in practice, but that can be used as test
          cases for different tools or approaches aimed at understanding the functioning of a more intricate transformer
          model.
        </p>

        <figure>
          <img src="images/tracr.png" class="center_image" style="width: 40%;">
          <figcaption>Caption: Toy models compiled to follow a known mechanism can be used as unit tests for
            interpretability tools.</figcaption>
          <figcaption>** Figure taken from <a href="https://arxiv.org/pdf/2301.05062.pdf" target="_blank">Lindner et
              al.
              2023</a> (Figure 1). <em>Tracr</em>
            is their compiler to compile
            RASP into a transformer.</figcaption>
        </figure>

        <br>
        <h4> Customize Transformers for Specific Tasks </h4>
        <p style="text-align: justify;">
          Although transformer architectures are being used in different situations, the current paradigm of how
          the components are ordered is somewhat consistent - attention and MLP are interleaved with attention followed
          by MLP in each transformer block, just as suggested by <a
            href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
            target="_blank">Vaswani et al 2017</a>.
          <a href="https://arxiv.org/abs/1911.03864" target="_blank">Press et al 2023</a> found that re-ordering
          MLP and attention modules can have significant implications on task performance. Specifically, they
          found that, provided there is some interleaving in the middle - 1) pushing MLPs towards earlier in the
          computation weakened the model's capability to learn language, 2) while pushing attentions towards earlier
          resulted in a stronger language model. This insight suggests that transformers with different architectures
          may be helpful in learning different tasks. And, RASP may help design such customized transformers and
          evaluate their differences. However, we will later discuss how
          RASP operations are not completely faithful to the components of a transformer block.
        </p>

        <br>
        <h4> An Educational Tool </h4>
        <p style="text-align: justify;">
          Solving different problems with RASP, may help develop intuitions on how different modules of a
          transformer work and also their respective capabilities. This may be useful as an educational tool for
          training researchers or industry practitioners interested in working with transformers.
        </p>

        <h2> Social Impact </h2>

        <p style="text-align: justify;">
          As of December 4, 2023, this paper has only 40 citations. It is impressive considering that the paper
          was published only two years ago, however it has not been "very impactful" yet. But, here is one scenario
          where this paper may end up have some impact (although this is very speculative) -
        </p>

        <p style="text-align: justify;">
          The impact of this paper may grow in future based on development in the field of interpretability. In
          future, there might be some interpretability tool that claims to understand a category of phenomena in
          models. A simple toy task under that category may be solved with a RASP program and then compiled into a
          transformer model. That model may be used to evaluate the efficacy of the interpretability tool. <br>

          As artificial models get increasingly capable and get more integrated into different aspects of our
          daily lives, policy-makers may want to make sure that these models are safe for deployment. And, this paper
          may have laid the groundwork for testing a subset of the tools that will be used to ensure the safety of
          artificial models.
        </p>


        <h2> Follow-on Research </h2>

        <h2> About the Authors </h2>

        <h2> Review </h2>

        <p style="text-align: justify;">
          This paper proposes a <em>restricted</em> programming language RASP, where a user can only use
          operations that are roughly analogous to the workings of different modules of a transformer model. The authors
          argue that using RASP will allow people to <em>"think like transformers"</em> and provide experimental
          evidence with some toy problems that human intuition can even be <em>compiled</em> into transformer models.
        </p>

        <p style="text-align: justify;">
          The strengths of this paper has been already discussed in previous sections - RASP can be used as an
          educational tool to devolop intuitions on how different components of a transformer block work. It may even
          inform us on how to customize different transformer architectures targeting specific problems. And, there
          might be a future
          where this approach can be used to evaluate a subset of tools used for ensuring that a transformer model is
          safe for deployment, although currently this is just a speculation by the authors of this blog.
        </p>

        <p style="text-align: justify;">
          A weakness of this work is that RASP operations are not completely faithful to how different
          components of a
          transformer block work in practice. For example:
        <ol style="text-align: justify;">
          <li>
            RASP completely ignores the Layernorms (<a href="https://arxiv.org/abs/1607.06450" target="_blank">Ba et
              al.
              2016</a>).
            Layernorms are used in transformers before each of the attention and MLP operations, and also finally before
            decoding the latent back to vocabulary space. This ensures stability in training and improves the
            performance (<a href="https://arxiv.org/abs/2305.02582" target="_blank">Brody et al. 2023</a>). Element-wise
            operations in RASP, which are analogous to MLP transformations, may be used to perform element-wise
            mathematical operations, such as; multiplications and divisions. But, the effect of Layernorms may wash out
            these transformations in practice.
          </li>

          <li>
            RASP makes the assumption that information stored in intermediate variables will be stored <em>as it is</em>
            in the residual stream. In theory this may be achieved by ensuring mutually orthogonal subspaces for all the
            intermediate variables. However, in practice transformers only have a limited number of dimensions in
            the residual stream. Thus it is most likely that subsequent attention and MLP modules may rewrite or even
            delete some of those variables.
          </li>

          <li>
            RASP emulates the attention pattern with a binary <em>selector</em> and as a result all the values that get
            <em>attended</em> by a query will get equal <em>attention</em>. In practice, however, the
            attention pattern has continuous values and attention paid to every single value can be different.
          </li>

        </ol>

        Despite certain limitations this paper can be lauded for the novelty of its main idea. Overall the paper
        was well presented and easy to follow.

        </p>

        <h3> References</h3>

        <!-- <p><a name="bottou-1990">[1]</a> <a href="https://papers.baulab.info/Bottou-1990.pdf"
            target="_blank">L&eacute;on Bottou and
            Patrick Gallinari.
            <em>A framework for the cooperation of learning algorithms.</em></a>
          Advances in neural information processing systems 3 (1990).
        </p> -->

        <p>
          <a name="lindner-2023">[2]</a>
          Lindner, David, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik.
          <a href="https://arxiv.org/abs/2301.05062">
            <em>"Tracr: Compiled transformers as a laboratory for interpretability."</em>
          </a> (2023)
        </p>

        <p>
          <a name="friedman-2023">[2]</a>
          Friedman, Dan, Alexander Wettig, and Danqi Chen.
          <a href="https://arxiv.org/abs/2306.01128">
            <em>"Learning Transformer Programs."</em>
          </a> (2023)
        </p>

        <p>
          <a name="raspy">[2]</a>
          Sasha Rush and Gail Weiss.
          <a href="https://srush.github.io/raspy/">
            <em>"RASPy"</em>
          </a>
        </p>

        <p>
          <a name="vaswani-2017">[2]</a>
          Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and
          Illia Polosukhin.
          <a
            href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">
            <em>"Attention is all you need."</em>
          </a> (2017)
        </p>

        <p>
          <a name="press-2019">[2]</a>
          Press, Ofir, Noah A. Smith, and Omer Levy.
          <a href="https://arxiv.org/abs/1911.03864">
            <em>"Improving transformer models by reordering their sublayers."</em>
          </a> (2019)
        </p>

        <p>
          <a name="ba-2016">[2]</a>
          Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton.
          <a href="https://arxiv.org/abs/1607.06450">
            <em>"Layer normalization."</em>
          </a> (2016)
        </p>

        <p>
          <a name="brody-2023">[2]</a>
          Brody, Shaked, Uri Alon, and Eran Yahav.
          <a href="https://arxiv.org/abs/2305.02582">
            <em>"On the Expressivity Role of LayerNorm in Transformers' Attention."</em>
          </a> (2023)
        </p>

        <p>
          <a name="conmy-2023">[2]</a>
          Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso.
          <a href="https://arxiv.org/abs/2304.14997">
            <em>"Towards Automated Circuit Discovery for Mechanistic Interpretability."</em>
          </a> (2023)
        </p>

        <h2>Team Members</h2>
        <ul>
          <li><a href="" target="_blank">David Atkinson</a></li>
          <li><a href="https://arnab-api.github.io/" target="_blank">Arnab Sen Sharma</a></li>
        </ul>

      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>