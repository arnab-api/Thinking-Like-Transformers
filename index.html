<!doctype html>
<html lang="en">

<head>
  <title>Thinking Like Transformers</title>
  <meta property="og:title" content="Thinking Like Transformers" />
  <meta name="twitter:title" content="Thinking Like Transformers" />
  <meta name="description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta property="og:description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta name="twitter:description" content="An analysis of Thinking Like Transformers paper by Weiss et al." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr"><a href="https://arxiv.org/abs/2106.06981" target="_blank">Thinking Like Transformers</a>
        </nobr>
        <!-- <nobr class="widenobr">For CS 7150</nobr> -->
      </h1>
    </div>
    <address>
      <nobr><a href="https://gailweiss.github.io/" target="_blank">Gail Weiss</a>,</nobr>
      <nobr><a href="https://u.cs.biu.ac.il/~yogo/" target="_blank">Yoav Goldberg</a>,</nobr>
      <nobr><a href="https://csaws.cs.technion.ac.il/~yahave/" target="_blank">Eran Yahav</a></nobr>
    </address>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <!-- <h2>An Analysis of [papername]</h2> -->
        <!-- <p>Describe the paper and the big question about it that interests you.</p> -->
        Analysis by <a href="https://diatkinson.github.io/" target="_blank">David Atkinson</a> and <a
          href="https://arnab-api.github.io/" target="_blank">Arnab Sen Sharma</a>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <!-- <h2>Literature Review; Biography; Social Impact; Industry Applications; Follow-on Research; and Peer-Review</h2>

        <p>Just as we have done in the role-playing exercise, analyze the paper from all perspectives.
        </p>

        <p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on
          your findings.
        </p> -->
        <h2> Introduction/Motivation </h2>
        <p style="text-align: justify;">
          Transformer models are famously opaque, made up of millions&mdash;sometimes billions&#8212;of uninterpretable
          parameters.
          It's natural to ask if there is a better representation of these parameters.
          <i>Thinking Like Transformers</i> argues that sometimes the answer is "yes," and that the better
          representation takes the form of a programming language called RASP, short for Restricted Access Sequence
          Processing Language.
        </p>
        <p style="text-align: justify;">
          RASP is a simple language designed to be &ldquo;compiled&rdquo; to transformer weights.
          Like transformers, RASP programs take sequences as input and produce different sequences as output.
          Again like transformers, the structure of the RASP language is three-fold:
        </p>
        <p style="text-align: justify;">
          First, every program starts with tokens (the input to a program, of the kind you might see in a context
          window) and indices (a sequence of integers between <i>0</i> and <i>n-1</i>, where <i>n</i> is the length of
          tokens).
          By analogy to the tokens and positional embeddings of a transformer, the tokens and indices of a RASP program
          combine to give sequential structure to user input.
        </p>
        <p style="text-align: justify;">
          The next kind of RASP program component is the element-wise operation, analogous to the MLP components of
          trained transformer.
          These will be familiar: we can use <span
            style="font-family: 'Courier New', Courier, monospace;"><b>+</b></span> to add two sequences; <span
            style="font-family: 'Courier New', Courier, monospace;"><b>pow(., .)</b></span> to exponentiate each element
          in a sequence; <span style="font-family: 'Courier New', Courier, monospace;"><b>&lt;</b></span>,<span
            style="font-family: 'Courier New', Courier, monospace;"><b>&gt;</b></span>,<span
            style="font-family: 'Courier New', Courier, monospace;"><b>&lt;=</b></span>,<span
            style="font-family: 'Courier New', Courier, monospace;"><b>&gt;=</b></span>, and <span
            style="font-family: 'Courier New', Courier, monospace;"><b>==</b></span> to express predicates over two
          elements; and so on.
        </p>
        <p style="text-align: justify;">
          Most importantly, we also have an attention mechanism analogue: the <span
            style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> and <span
            style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span> operations.
          Although a fuller description can be found <a href="#diagram">below</a>, at a high level, <span
            style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> combines
          two sequences with a predicate, generating a matrix containing the result of applying the predicate to every
          possible combination of sequence elements. <span
            style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>,
          subsequently, will combine that resulting matrix with a third sequence to produce a new sequence. If you
          squint, you can see how the key and query vectors of the transformer's attention mechanism are treated
          similarly to the inputs to <span
            style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span>, and how
          the value vectors of the transformer are analogous to the the sequence input of <span
            style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>.
        </p>
        <p style="text-align: justify;">
          What can you actually do with RASP? A few things. First, having constructed a RASP program, you have also
          established an upper bound on the number of layers and attention heads needed to express your algorithm.
          Second, and more speculatively, you might think you've improved your intuitive sense of what a trained model
          might be doing internally when it performs the task expressed by your program. (The authors find that a
          trained model will sometimes, but not always, embody the algorithm implied by the equivalent RASP program.)
          Finally, RASP can be used as "laboratory for interpretability." <a href="#lindner-2023">[1]</a> When trained
          transformer models are interpreted, it is often unclear when or how full success is achieved. Using RASP
          programs as interpretation targets can serve as a ground truth for interpretability methods. <a
            href="#conmy-2023">[2]</a>
        </p>


        <h2 id="diagram"> <b>R</b>estricted <b>A</b>ccess <b>S</b>equence <b>P</b>rocessing (RASP) </h2>

        <br>
        <h4> Analogues to components in a transformer block</h4>

        <figure>
          <img src="images/rasp_ops.png" class="center_image" style="width: 45%;">
          <figcaption>Caption: Components of a Transformer block and their RASP analogues.</figcaption>
        </figure>

        <br>
        <h5> Sequence Operators (S-Ops)</h5>
        <p style="text-align: justify;">

          S-ops in RASP are operations that take a sequence as input and generate a sequence of the same length as
          output. Functions of MLP and attention blocks in a transformer are emulated as S-Ops in RASP. RASP also
          provides two built-in S-Ops: <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              <span style="font-family: 'Courier New', Courier, monospace;">
                &nbsp; tokens("hello") = [h, e, l, l, o] <br>
                &nbsp; indices("hello") = [0, 1, 2, 3, 4] <br>
              </span>
            </p>
          </div>
        </div>

        </p>

        <br>
        <h5> Element-wise Operators </h5>
        <p style="text-align: justify;">

          These operations are analogous to element-wise transformations performed by an MLPs.
        <figure>
          <img src="images/mlp_op.png" class="center_image" style="width: 35%;">
        </figure>
        <br>

        An example of such operations will be: <br><br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              <span style="font-family: 'Courier New', Courier, monospace;">
                &nbsp; 3 * indices("hello") = [0, 3, 6, 9, 12] <br>
              </span>
            </p>
          </div>
        </div>

        </p>

        <br>
        <h5> Select and Aggregate Operations </h5>
        <p style="text-align: justify;">
          RASP emulates the mechanism of a single attention head with a combination of
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span> and
          <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate()</b></span>
          operations.
        </p>

        <p style="text-align: justify;">
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span>
          takes two sequences; the
          <span style="color: darkorange; font-family: 'Courier New', Courier, monospace;"><b>key</b></span>
          and the
          <span style="color: teal; font-family: 'Courier New', Courier, monospace;"><b>query</b></span>
          as input, and generates a <em>binary</em> mask using
          them. In RASP, this binary mask is called a <em>selector</em>. <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              &nbsp &nbsp
              <span style="font-family: 'Courier New', Courier, monospace;">
                <span style="color: palevioletred;"><b>select(</b></span>
                <span style="color: darkorange;"><b>key</b></span>
                <span style="color: palevioletred;"><b>,</b></span>
                <span style="color: teal;"><b>query</b></span>
                <span style="color: palevioletred;"><b>,</b></span>
                <b>binary op</b>
                <span style="color: palevioletred;"><b>)</b></span>
              </span>
            </p>
          </div>
        </div>

        <br><br>
        The <em>selector</em> mask is be a square matrix with binary values that is analogous to the attention
        pattern of a single attention head.<br>

        The value in cell <span style="font-family: 'Courier New', Courier, monospace;">(i, j)</span> of
        <em>seletor</em> is
        decided as:

        &nbsp
        <span style="font-family: 'Courier New', Courier, monospace;">
          <span style="color: black;"><b>selector(i, j) = </b></span>
          <span style="color: teal;"><b>query(i)</b></span>
          <span style="color: black;"><b>op</b></span>
          <span style="color: darkorange;"><b>key(j)</b></span>
        </span>
        <br><br>

        To illustrate this,
        <span style="font-family: 'Courier New', Courier, monospace;">
          <span style="color: palevioletred;"><b>select(</b></span>
          <span style="color: darkorange;"><b>key = [1, 2, 3, 4, 5]</b></span>
          <span style="color: palevioletred;"><b>,</b></span>
          <span style="color: teal;"><b>query = [0, 1, 2, 3, 4]</b></span>
          <span style="color: palevioletred;"><b>,</b></span>
          <b>>=</b>
          <span style="color: palevioletred;"><b>)</b></span>
        </span>
        will generate the following <em>selector</em> - <br>

        <figure>
          <img src="images/selector_example.png" class="center_image" style="width: 20%;">
        </figure>

        In RASP, <span
          style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select()</b></span> is always
        followed by an
        <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate()</b></span>
        operation, which
        takes a
        <span style="font-family: 'Courier New', Courier, monospace;"><b>selector</b></span> matrix and a
        <span style="color: red; font-family: 'Courier New', Courier, monospace;"><b>value</b></span> sequence as input,
        like below - <br> <br>

        <div class="card">
          <!-- <h3 class="card-header">bibliography</h3> -->
          <div class="card-block">
            <p class="card-text clickselect">
              &nbsp &nbsp
              <span style="font-family: 'Courier New', Courier, monospace;">
                <span style="color: darkred;"><b>aggregate(</b></span>
                <b>selector</b>
                <span style="color: darkred;"><b>,</b></span>
                <span style="color: red;"><b>value</b></span>
                <span style="color: darkred;"><b>)</b></span>
              </span>
            </p>
          </div>
        </div>

        <br>

        And, generates a sequence of the same length by aggregating the values in the <span
          style="color:red; font-family: 'Courier New', Courier, monospace;"><b>value</b></span> sequence as per
        the <em>mixture</em>
        defined in the rows of the <span style="font-family: 'Courier New', Courier, monospace;"><b>selector</b></span>
        matrix. <br>

        <br>
        <figure>
          <img src="images/aggregate_eqn.png" class="center_image" style="width: 80%;">
        </figure>
        <br>

        From here on, we will borrow the following concise diagram from <i>RASPy</i> <a href="#raspy">[3]</a> to
        illustrate how a single attention head is
        emulated
        using a pair of
        <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> -
        <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>
        operations. <br>

        <br>
        <figure>
          <img src="images/rasp_attn.png" class="center_image" style="width: 65%;">
        </figure>
        <br>

        </p>

        <div class="alert error">
          <span class="alertText" style="color:darkred;">
            ** A RASP selector is a square matrix of binary values, whereas an attention pattern has
            continuous values. Also, in an attention pattern, values in a row must sum to 1 due to the softmax
            applied on scaled attention. RASP doesn't impose this on a selector either.</li>
          </span>
        </div>

        <div class="alert warning">
          <span class="alertText" style="color:black; text-align: left;">
            ** In RASP, values are aggregated as the mean. However, in this analysis we show aggregation as sum,
            similar to the implementation of <i>RASPy</i> <a href="#raspy">[3]</a>.
            </ul>
          </span>
        </div>



        <br>
        <h4> Some Illustrative Examples </h4>

        <br>
        <h5> Example 1: Count the number of times a character c appears in a sequence S </h5>

        Input Format: <span style="font-family:'Courier New'">
          &ltsequence&gt-&ltc&gt
        </span> <br>

        RASP Program:
        <br>
        <br>
        <figure>
          <img src="images/char_rasp.png" style="width: 50%;">
        </figure>
        <br>

        <p style="text-align: justify;">
          The figure below illustrates each step for an input <span style="font-family:'Courier New'">"abaa-a"</span>.
          This implementation will require 3 transformer
          layers. Each of the attention and MLP operations will add one set of new information to the residual streams.
          The answer will be written after Attn_3.
        </p>

        <br>
        <figure>
          <img src="images/char_occurance.png" class="center_image" style="width: 100%;">
        </figure>
        <br>

        <br>
        <h5> Example 2: Find the minimum element in a sequence </h5>

        Idea: Given a sequence <span style="font-family:'Courier New'">S = [3, 2, 3, 5, 2]</span>, we will calculate 2
        counts as follows -

        <ol>
          <li>
            For each <span style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>,
            count the
            number of elements in S that are less than <span
              style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>
            <figure>
              <img src="images/count_less.png" style="width: 30%;">
            </figure>
          </li>
          <li>
            Also, count the number of elements that appear before <span
              style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span> and is equal to
            <span style="font-family: Georgia, 'Times New Roman', Times, serif;"><i>x<sub>i</sub></i></span>
            <figure>
              <img src="images/count_before_eq.png" style="width: 40%;">
            </figure>
          </li>
        </ol>

        For the example sequence, <span style="font-family:'Courier New'">S = [3, 2, 3, 5, 2]</span>, the counts will
        be<br>

        <br>
        <span style="font-family:'Courier New'">count_less = [2, 0, 2, 4, 0]</span> <br>
        <span style="font-family:'Courier New'">count_before_eq = [0, 0, 1, 0, 1]</span> <br>
        And, <span style="font-family:'Courier New'">count = count_less + count_before_eq = [2, 0, 3, 4, 1]</span> <br>
        <br>

        There will only be one zero in the <span style="font-family:'Courier New'">count</span> sequence and it will be
        on the index where the minimum element appears first in S.<br>
        The RASP code with operations and updates at each step is illustrated below.

        <br>
        <br>
        <figure>
          <img src="images/minimum.png" class="center_image" style="width: 70%;">
        </figure>
        <br>


        <br>
        <br>
        <h4> Compiling RASP into a transformer model </h4>
        <p style="text-align: justify;">
          Once we have a RASP program that solves a certain task, it is possible to compile that RASP program into an
          actual transformer model. The longest path in the RASP program from input to output decides the number of
          transformer layers. And, the number of independent
          <span style="color: palevioletred; font-family: 'Courier New', Courier, monospace;"><b>select</b></span> -
          <span style="color: darkred; font-family: 'Courier New', Courier, monospace;"><b>aggregate</b></span>
          operations being done in parallel decides the number of attention heads per layer.
        </p>
        <p style="text-align: justify;">
          After we decide on a transformer architecture, the model can be trained with supervision from -
        <ol>
          <li>An dataset with input-output pairs that demonstrates the task, how any supervised model is usually
            trained.</li>
          <li>Attention patterns of each of the heads are matched to a corresponding <em>selector</em> pattern
            in the RASP program.</li>
        </ol>

        <figure>
          <img src="images/compiling.png" class="center_image" style="width: 90%;">
          <figcaption>(a) shows the RASP program that solves the double histogram problem. The program uses
            three
            attention operators. (b) shows the selector pattern for the 3 attention operations for an input
            sequence
            <span style="font-family:'Courier New'">"Â§aaabbccdef"</span>. (c) shows the corresponding attention
            heatmaps
            for a transformer model trained to solve the
            task with the attention pattern specified by the selectors of the RASP program.
          </figcaption>
        </figure>

        Two followup works by Lindner et al, 2023 <a href="#lindner-2023">[1]</a>
        and
        Friedman et al, 2023
        <a href="#friedman-2023">[2]</a>
        presented
        compilers that
        generate a transformer
        model given a RASP program and a set of input-output demonstration pairs.

        </p>

        <h2> Related Works </h2>

        <p style="text-align: justify;">
          Prior to this work, expressing RNNs as automata was an active line of research, seemingly inaugurated by <a
            href="#giles-1992">[1]</a>. Certain problem formulations provided access to training examples, while others
          allowed access to model weights or activations <a href="#omlin-1996">[2]</a>, and still others constrained the
          extractor to use model outputs only <a href="#weiss-2018">[3]</a> <a href="#ayache-2019">[4]</a>. Similarly,
          using automata to characterize the expressive power of RNNs was tackled in <a href="#rabusseau-2019">[5]</a>,
          which focused on second-order RNNs, and in <a href="#merrill-2019">[6]</a> and <a
            href="#merrill-2020">[7]</a>, both of which restricted their attention to &ldquo;saturated&rdquo; RNNs, or
          RNNs in which all the activation functions have been replaced with step functions.
        </p>
        <p style="text-align: justify;">
          At the time this paper was published, analogous work for transformers was more tightly-scoped: <a
            href="#yun-2019">[8]</a> showed that transformers are expressive enough to approximate arbitrary continuous
          sequence-to-sequence functions; <a href="#hahn-2020">[9]</a>, by contrast, showed that fixed-size transformers
          are unable to model both periodic finite-state languages and hierarchical structure; finally, <a
            href="#perez-2021">[10]</a> showed that some transformer variants are Turing complete.
        </p>


        <h2> Applications </h2>
        <br>
        <h4> Testing Interpretability Tools </h4>
        <p style="text-align: justify;">
          Interpretability is a budding field that seeks to understand how neural networks work by <em>decompiling</em>
          them into human-readable algorithm or program. Weiss et al. here take an opposite approach in this paper -
          solving a particular problem with RASP that only allows operations analogous to different components of a
          transformer. This RASP solution can be <em>compiled</em> into a transformer model that is interpretable
          by design. These interpretable toy models may not be very useful in practice, but that can be used as test
          cases for different tools or approaches aimed at understanding the functioning of a more intricate transformer
          model.
        </p>

        <figure>
          <img src="images/tracr.png" class="center_image" style="width: 40%;">
          <figcaption>Caption: Toy models compiled to follow a known mechanism can be used as unit tests for
            interpretability tools.</figcaption>
          <figcaption>** Figure taken from <a href="https://arxiv.org/pdf/2301.05062.pdf" target="_blank">Lindner et
              al.
              2023</a> (Figure 1). <em>Tracr</em>
            is their compiler to compile
            RASP into a transformer.</figcaption>
        </figure>

        <br>
        <h4> Customize Transformers for Specific Tasks </h4>
        <p style="text-align: justify;">
          Although transformer architectures are being used in different situations, the current paradigm of how
          the components are ordered is somewhat consistent - attention and MLP are interleaved with attention followed
          by MLP in each transformer block, just as suggested by <a
            href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
            target="_blank">Vaswani et al 2017</a>.
          <a href="https://arxiv.org/abs/1911.03864" target="_blank">Press et al 2023</a> found that re-ordering
          MLP and attention modules can have significant implications on task performance. Specifically, they
          found that, provided there is some interleaving in the middle - 1) pushing MLPs towards earlier in the
          computation weakened the model's capability to learn language, 2) while pushing attentions towards earlier
          resulted in a stronger language model. This insight suggests that transformers with different architectures
          may be helpful in learning different tasks. And, RASP may help design such customized transformers and
          evaluate their differences. However, we will later discuss how
          RASP operations are not completely faithful to the components of a transformer block.
        </p>

        <br>
        <h4> An Educational Tool </h4>
        <p style="text-align: justify;">
          Solving different problems with RASP, may help develop intuitions on how different modules of a
          transformer work and also their respective capabilities. This may be useful as an educational tool for
          training researchers or industry practitioners interested in working with transformers.
        </p>

        <h2> Social Impact </h2>

        <p style="text-align: justify;">
          As of December 4, 2023, this paper has only 40 citations. It is impressive considering that the paper
          was published only two years ago, however it has not been "very impactful" yet. But, here is one scenario
          where this paper may end up have some impact (although this is very speculative) -
        </p>

        <p style="text-align: justify;">
          The impact of this paper may grow in future based on development in the field of interpretability. In
          future, there might be some interpretability tool that claims to understand a category of phenomena in
          models. A simple toy task under that category may be solved with a RASP program and then compiled into a
          transformer model. That model may be used to evaluate the efficacy of the interpretability tool. <br>

          As artificial models get increasingly capable and get more integrated into different aspects of our
          daily lives, policy-makers may want to make sure that these models are safe for deployment. And, this paper
          may have laid the groundwork for testing a subset of the tools that will be used to ensure the safety of
          artificial models.
        </p>


        <h2> Follow-on Research </h2>

        We suggest two possible avenues of further research:

        <h3>When does gradient descent find a RASP program?</h3>

        <p style="text-align: justify;">
          Strikingly, the authors observe an example of a model trained on the Histogram-BOS task ultimately matching
          the attention pattern of the equivalent RASP program. How often&mdash;and under which conditions&mdash;does
          this happen?
        </p>
        <p style="text-align: justify;">
          There are many of ways to approach this, but one might involve generating random RASP programs as targets, and
          then training transformer architectures chosen to match the targets while varying the random seed. How often
          does SGD find an equivalent attention pattern? How does that vary with model size or program type? The extent
          to which RASP is a useful avenue of further research is closely related to its ability to mimic the inductive
          biases of real-world transformer models, and the hope is that this research agenda could shed light on that.
        </p>
        <h3>Can RASP inspire improvements to the transformer architecture?</h3>
        </p>
        <p style="text-align: justify;">
          One of the ways Weiss et al. demonstrate the value of RASP is by describing its ability to explain the
          findings of <a href="#press-2020">[1]</a>. In that paper, the authors show that a transformer architecture
          which has more attention sublayers early in the model, and more MLP sublayers later, outperforms standard
          transformers on language modeling tasks. Weiss et al. explain this with reference to RASP:
        </p>
        <p style="text-align: justify;">
        <blockquote>"In RASP, there is little value to repeated elementwise operations before the first aggregate: each
          position has only its initial input, and cannot generate new information... In contrast, an architecture
          beginning with several attention sublayers&mdash;i.e., multiple select-aggregate pairs&mdash;will be able to
          gather a large amount of information into each position early in the computation, even if only by simple
          rules"</blockquote>
        </p>
        <p style="text-align: justify;">
          Although this kind of reasoning is prompted by existing architectures, the process could in principle run in
          reverse: perhaps there are lessons to be taken from the structure of RASP programs which are relevant to
          designing better "transformer-esque architectures?"
        </p>
        <p style="text-align: justify;">
          An example of this process might take inspiration from the findings of <a href="deletang-2022">[2]</a> which
          identified tasks which a transformer could theoretically solve, but for which empirical training did not
          converge to a solution. RASP programs for those tasks could shed light on the kinds of architectures with the
          right inductive bias for those tasks.
        </p>

        <h2> About the Authors </h2>

        <p style="text-align: justify;">
          The first author of the paper, Gail Weiss, was a PhD student at the Technion when the work was completed.
          Advised by the other two authors, she focuses mostly on the intersection of neural networks and formal
          language theory. (She is the founder, for example, of the Formal Languages and Neural Networks (FLaNN) <a
            href="https://flann.super.site/">Discord</a>.) Today, she is a postdoc at EPFL, in France.
        </p>
        <p style="text-align: justify;">
          Yoav Goldberg&mdash;also an Israeli computer scientist, although in this case, a professor at Bar Ilan
          University&mdash;was an early participant in NLP&apos;s embrace of neural networks. This led to a 2017 <a
            href="https://link.springer.com/book/10.1007/978-3-031-02165-7">book</a>, <i>Neural Network Methods for
            Natural Language Processing</i> and a series of highly cited papers in the mid-to-early 2010s attempting to
          understand word embeddings. In addition to his academic position, he also serves as Research Director at the
          Israeli branch of the Allen Institute for AI. He received his PhD at Ben-Gurion University, went on to a
          postdoc at Google Research, and sells t-shirts on the internet, available for sale <a
            href="https://www.zazzle.com/store/yoavgo">right now</a>.
        </p>
        <p style="text-align: justify;">
          The final author, Eran Yahav, is a professor at Technion and CTO at TabNine (an early LLM-based coding
          assistant). His previous life is a bit of a mystery, although he was, at some point, affiliated with Tel-Aviv
          University. In his current life, he has written many machine learning papers on program analysis, program
          synthesis, and programming languages.
        </p>

        <h2> Review </h2>

        <p style="text-align: justify;">
          This paper proposes a <em>restricted</em> programming language RASP, where a user can only use
          operations that are roughly analogous to the workings of different modules of a transformer model. The authors
          argue that using RASP will allow people to <em>"think like transformers"</em> and provide experimental
          evidence with some toy problems that human intuition can even be <em>compiled</em> into transformer models.
        </p>

        <p style="text-align: justify;">
          The strengths of this paper has been already discussed in previous sections - RASP can be used as an
          educational tool to devolop intuitions on how different components of a transformer block work. It may even
          inform us on how to customize different transformer architectures targeting specific problems. And, there
          might be a future
          where this approach can be used to evaluate a subset of tools used for ensuring that a transformer model is
          safe for deployment, although currently this is just a speculation by the authors of this blog.
        </p>

        <p style="text-align: justify;">
          A weakness of this work is that RASP operations are not completely faithful to how different
          components of a
          transformer block work in practice. For example:
        <ol style="text-align: justify;">
          <li>
            RASP completely ignores the Layernorms (<a href="https://arxiv.org/abs/1607.06450" target="_blank">Ba et
              al.
              2016</a>).
            Layernorms are used in transformers before each of the attention and MLP operations, and also finally before
            decoding the latent back to vocabulary space. This ensures stability in training and improves the
            performance (<a href="https://arxiv.org/abs/2305.02582" target="_blank">Brody et al. 2023</a>). Element-wise
            operations in RASP, which are analogous to MLP transformations, may be used to perform element-wise
            mathematical operations, such as; multiplications and divisions. But, the effect of Layernorms may wash out
            these transformations in practice.
          </li>

          <li>
            RASP makes the assumption that information stored in intermediate variables will be stored <em>as it is</em>
            in the residual stream. In theory this may be achieved by ensuring mutually orthogonal subspaces for all the
            intermediate variables. However, in practice transformers only have a limited number of dimensions in
            the residual stream. Thus it is most likely that subsequent attention and MLP modules may rewrite or even
            delete some of those variables.
          </li>

          <li>
            RASP emulates the attention pattern with a binary <em>selector</em> and as a result all the values that get
            <em>attended</em> by a query will get equal <em>attention</em>. In practice, however, the
            attention pattern has continuous values and attention paid to every single value can be different.
          </li>

        </ol>

        Despite certain limitations this paper can be lauded for the novelty of its main idea. Overall the paper
        was well presented and easy to follow.

        </p>

        <h3> References</h3>

        <!-- <p><a name="bottou-1990">[1]</a> <a href="https://papers.baulab.info/Bottou-1990.pdf"
            target="_blank">L&eacute;on Bottou and
            Patrick Gallinari.
            <em>A framework for the cooperation of learning algorithms.</em></a>
          Advances in neural information processing systems 3 (1990).
        </p> -->

        <p>
          <a name="lindner-2023">[1]</a>
          Lindner, David, J&aacute;nos Kram&aacute;r, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik.
          <a href="https://arxiv.org/abs/2301.05062">
            <em>"Tracr: Compiled transformers as a laboratory for interpretability."</em>
          </a> (2023)
        </p>

        <p>
          <a name="friedman-2023">[2]</a>
          Friedman, Dan, Alexander Wettig, and Danqi Chen.
          <a href="https://arxiv.org/abs/2306.01128">
            <em>"Learning Transformer Programs."</em>
          </a> (2023)
        </p>

        <p>
          <a name="raspy">[3]</a>
          Sasha Rush and Gail Weiss.
          <a href="https://srush.github.io/raspy/">
            <em>"RASPy"</em>
          </a>
        </p>

        <p>
          <a name="vaswani-2017">[4]</a>
          Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, &Lstrok;ukasz
          Kaiser, and
          Illia Polosukhin.
          <a
            href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">
            <em>"Attention is all you need."</em>
          </a> (2017)
        </p>

        <p>
          <a name="press-2019">[5]</a>
          Press, Ofir, Noah A. Smith, and Omer Levy.
          <a href="https://arxiv.org/abs/1911.03864">
            <em>"Improving transformer models by reordering their sublayers."</em>
          </a> (2019)
        </p>

        <p>
          <a name="ba-2016">[6]</a>
          Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton.
          <a href="https://arxiv.org/abs/1607.06450">
            <em>"Layer normalization."</em>
          </a> (2016)
        </p>

        <p>
          <a name="brody-2023">[7]</a>
          Brody, Shaked, Uri Alon, and Eran Yahav.
          <a href="https://arxiv.org/abs/2305.02582">
            <em>"On the Expressivity Role of LayerNorm in Transformers' Attention."</em>
          </a> (2023)
        </p>

        <p>
          <a name="conmy-2023">[8]</a>
          Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri&aacute; Garriga-Alonso.
          <a href="https://arxiv.org/abs/2304.14997">
            <em>"Towards Automated Circuit Discovery for Mechanistic Interpretability."</em>
          </a> (2023)
        </p>

        <p>
          <a name="giles-1992">[9]</a>
          C. L. Giles, C. B. Miller, D. Chen, H. H. Chen, G. Z. Sun, and Y. C. Lee.
          <a
            href="https://direct.mit.edu/neco/article-abstract/4/3/393/5641/Learning-and-Extracting-Finite-State-Automata-with?redirectedFrom=fulltext">
            <em>"Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks."</em>
          </a> (1992)
        </p>

        <p>
          <a name="omlin-1996">[10]</a>
          Christian W. Omlin, and C.Lee Giles.
          <a href="https://www.sciencedirect.com/science/article/abs/pii/0893608095000860?via%3Dihub">
            <em>"Extraction of rules from discrete-time recurrent neural networks."</em>
          </a> (1996)
        </p>

        <p>
          <a name="weiss-2018">[11]</a>
          Gail Weiss, Yoav Goldberg, and Eran Yahav.
          <a href="https://proceedings.mlr.press/v80/weiss18a.html">
            <em>"Extracting automata from recurrent neural networks using queries and counterexamples."</em>
          </a> (2018)
        </p>

        <p>
          <a name="ayache-2019">[12]</a>
          Stephane Ayache, Remi Eyraud, and Noe Goudian.
          <a href="https://proceedings.mlr.press/v93/ayache19a/ayache19a.pdf">
            <em>"Explaining Black Boxes on Sequential Data using Weighted Automata."</em>
          </a> (2019)
        </p>

        <p>
          <a name="rabusseau-2019">[13]</a>
          Guillaume Rabusseau, Tianyu Li, and Doina Precup.
          <a href="https://proceedings.mlr.press/v89/rabusseau19a.html">
            <em>"Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning."</em>
          </a> (2019)
        </p>

        <p>
          <a name="merrill-2019">[14]</a>
          William Merrill.
          <a href="https://arxiv.org/abs/1906.01615">
            <em>"Sequential Neural Networks as Automata."</em>
          </a> (2019)
        </p>

        <p>
          <a name="merrill-2020">[15]</a>
          William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, and Eran Yahav.
          <a href="https://aclanthology.org/2020.acl-main.43/">
            <em>"A Formal Hierarchy of RNN Architectures."</em>
          </a> (2020)
        </p>

        <p>
          <a name="yun-2019">[16]</a>
          Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar.
          <a href="https://arxiv.org/abs/1912.10077">
            <em>"Are Transformers universal approximators of sequence-to-sequence functions?."</em>
          </a> (2019)
        </p>

        <p>
          <a name="hahn-2020">[17]</a>
          Michael Hahn.
          <a href="https://transacl.org/ojs/index.php/tacl/article/view/1815">
            <em>"Theoretical Limitations of Self-Attention in Neural Sequence Models."</em>
          </a> (2020)
        </p>

        <p>
          <a name="perez-2021">[18]</a>
          Jorge P&eacute;rez, Pablo Barcel&oacute;, Javier Marinkovic.
          <a href="https://jmlr.org/papers/v22/20-302.html">
            <em>"Attention is Turing-Complete."</em>
          </a> (2021)
        </p>

        <p>
          <a name="deletang-2022">[19]</a>
          Gr&eacute;goire Del&eacute;tang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt,
          Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega.
          <a href="https://arxiv.org/abs/2207.02098">
            <em>"Neural Networks and the Chomsky Hierarchy."</em>
          </a> (2022)
        </p>

        <h2>Team Members</h2>
        <ul>
          <li><a href="https://diatkinson.github.io/" target="_blank">David Atkinson</a></li>
          <li><a href="https://arnab-api.github.io/" target="_blank">Arnab Sen Sharma</a></li>
        </ul>

      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>